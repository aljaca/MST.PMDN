\name{MST.PMDN-package}
\alias{MST.PMDN-package}
\docType{package}
\title{Deep Multivariate Skew t-Parsimonious Mixture Density Network}
\description{
The \pkg{MST.PMDN} package provides a distributional deep learning regression framework
built on \pkg{torch} for R. Parameters of a mixture of multivariate skew t distributions
that describe a  multivariate output are estimated by training a deep learning model
with two  multi-modal input branches, one for tabular inputs and the other for (optional) 
image inputs, both appropriately scaled. The two branches are provided as 
user-defined \pkg{torch} modules. Outputs from each are concatenated and passed 
through a dense fusion network, which then leads to the MST-PMDN head. In the 
absence of both branches, the tabular inputs are fed directly into the dense 
network. Following the approach used in model-based clustering (\pkg{mclust}), 
scale matrices output from the MST-PMDN head are represented using a volume 
(L)-shape (A)-orientation (D) (LAD) eigenvalue decomposition parameterization. 
LAD attributes, the nu (or degrees of freedom) parameter (n), and the alpha (or 
skewness) parameter (s) can be forced to be Variable or Equal between mixture 
components (plus Identity for A and D). For n and s, values of nu and alpha can 
also be constrained to emulate a multivariate normal (N) distribution. In the 
case of n, users can specify fixed (F) values for nu by passing an optional 
\code{fixed_nu} vector. If an element of \code{fixed_nu} is set to \code{NA}, 
then the value of nu for this component is learned by the network. Different 
model types are specified by setting the argument \code{constraint = "EIINN"}, 
\code{"VEVFV"}, etc. where each letter position in the argument corresponds, 
respectively, to each of the LADns attributes. Furthermore, values of the mu (or 
means) (m), pi (or mixing coefficients) (x), volume-shape-orientation attributes 
(LAD), nu (or degrees of freedom) (n), and alpha (or skewness) (s) for the 
mixtures can be made to be independent of inputs by specifying any combination 
of \code{constant_attr = "m"}, \code{"mx"}, ..., \code{"LADmxns"}.

The deep MST-PMDN model is a framework for conditional multivariate 
density estimation that represents complicated joint output 
distributions as mixtures of skew-t components. The LAD decomposition 
allows for a tractable, interpretable, and parsimonious 
parameterization of the scale matrices, while explicit modeling of 
skewness and heavy tails can represent asymmetric behavior and tail 
dependence observed in real-world data. The dual-branch architecture 
fuses tabular and optional image inputs, making MST-PMDN applicable to 
diverse tasks where accurate uncertainty quantification and realistic 
ensemble generation are critical. Flexible constraint settings (e.g., 
fixed or learnable nu, normal approximations, shared vs. 
component-specific attributes) allow users to trade off parsimony and 
expressiveness, while the \pkg{torch} implementation delivers 
GPU-accelerated training.
}
\details{
Key user-facing functions include:
\itemize{
  \item \code{\link{define_mst_pmdn}}: construct a new MST-PMDN model object.
  \item \code{\link{train_mst_pmdn}}: fit an MST-PMDN model to training data.
  \item \code{\link{predict_mst_pmdn}}: generate predictive distributions or point predictions.
  \item \code{\link{loss_mst_pmdn}}: compute the negative log-likelihood loss for evaluation.
  \item \code{\link{sample_mst_pmdn}}: draw samples as \code{\link[torch]{torch_tensor}} objects.
  \item \code{\link{sample_mst_pmdn_df}}: draw samples returned as an R \code{\link{data.frame}}.
}
}
\references{
Ambrogioni, L., Güçlü, U., van Gerven, M. A., & Maris, E. (2017). The kernel mixture network: A nonparametric method for conditional density estimation of continuous random variables. arXiv:1705.07111. 
 
Andrews, J. L., & McNicholas, P. D. (2012). Model-based clustering, classification, and discriminant analysis via mixtures of multivariate t-distributions: the t EIGEN family. Statistics and Computing, 22, 1021-1029. 
 
Azzalini, A., & Capitanio, A. (2003). Distributions generated by perturbation of symmetry with emphasis on a multivariate skew t-distribution. Journal of the Royal Statistical Society Series B: Statistical Methodology, 65(2), 367-389. 
 
Andrews, J. L., Wickins, J. R., Boers, N. M., & McNicholas, P. D. (2018). teigen: An R package for model-based clustering and classification via the multivariate t distribution. Journal of Statistical Software, 83, 1-32. 
 
Banfield, J. D., & Raftery, A. E. (1993). Model-based Gaussian and non-Gaussian clustering. Biometrics, 803-821. 
 
Celeux, G., & Govaert, G. (1995). Gaussian parsimonious clustering models. Pattern Recognition, 28(5), 781-793. 

Falbel D., & Luraschi, J. (2025). torch: Tensors and Neural Networks with 'GPU' Acceleration. R package version 0.14.2, https://github.com/mlverse/torch, https://torch.mlverse.org/docs.

Fraley, C., & Raftery, A. E. (2002). Model-based clustering, discriminant analysis, and density estimation. Journal of the American Statistical Association, 97(458), 611-631. 
 
Fraley, C., & Raftery, A. E. (1998). How many clusters? Which clustering method? Answers via model-based cluster analysis. The Computer Journal, 41(8), 578-588. 
 
Lee, S., & McLachlan, G. J. (2014). Finite mixtures of multivariate skew t-distributions: some recent and new results. Statistics and Computing, 24, 181-202. 

Kingma, D. P., & Ba, J. (2015). Adam: a method for stochastic optimization. Proceedings of the 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA. arXiv:1412.6980 

Klein, N. (2024). Distributional regression for data analysis. Annual Review of Statistics and Its Application, 11:321-346.

Peel, D., & McLachlan, G.J. (2000). Robust mixture modelling using the t distribution. Statistics and Computing 10, 339–348. 

Srucca, L., Fop, M., Murphy, T. B., & Raftery, A. E. (2016). mclust 5: Clustering, classification and density estimation using Gaussian finite mixture models. The R Journal, 8(1), 289-317. 
 
Williams, P. M. (1996). Using neural networks to model conditional multivariate densities. Neural Computation, 8(4), 843-854.
}
\keyword{package}
